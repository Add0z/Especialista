# Kafka Ingestion Benchmark: High-Performance CSV to Topic

Este projeto apresenta uma an√°lise de performance e implementa√ß√£o de uma pipeline de ingest√£o massiva de dados, movendo mais de 5,2 milh√µes de registros de um arquivo CSV para o Apache Kafka. O foco principal foi o equil√≠brio entre Throughput (vaz√£o), utiliza√ß√£o de CPU e efici√™ncia de I/O.

## üöÄ Resultados do Benchmark

* **Volume de Dados:** 5.208.181 registros (CSV).
* **Throughput M√°ximo (12 Parti√ß√µes):** **361.002 registros/segundo**.
* **Throughput (6 Parti√ß√µes):** **342.345 registros/segundo**.
* **Tempo Total (12 Parti√ß√µes):** ~14,4 segundos.
* **Tempo Total (6 Parti√ß√µes):** ~15,2 segundos.
* **Stack:** Java 21, Spring Boot 3.x, Apache Kafka.

---

## üß† Decis√µes Arquiteturais e An√°lise de Performance

### 1. O Custo Oculto do `substring()` e Aloca√ß√£o de Objetos

Uma das principais frentes de otimiza√ß√£o foi o m√©todo de parsing. Inicialmente, avaliamos um parser manual utilizando `substring()` para extrair cada token.

* **O Problema:** O uso extensivo de `line.substring(start, i)` dentro de um loop para 5.2 milh√µes de linhas gera uma press√£o imensa no Garbage Collector (GC). Cada chamada cria um novo objeto `String` na Heap.
* **A Solu√ß√£o:** Evolu√≠mos para um modelo onde priorizamos o tratamento direto de `byte[]` sempre que poss√≠vel, reduzindo a cria√ß√£o de objetos tempor√°rios e mantendo a CPU focada no envio de dados, n√£o na limpeza de mem√≥ria.

### 2. `byte[]` vs. Serializa√ß√£o Bin√°ria (Pseudo-Avro)

Comparamos o envio de dados como `byte[]` contra uma simula√ß√£o de **Avro (bin√°rio)**.

* **Resultado:** Em ambiente **localhost**, o custo de CPU para converter tipos (ex: `Double.parseDouble()`) via c√≥digo Java superou o ganho de economia de rede. A abordagem de bytes brutos provou-se o "teto" de performance local, atingindo o pico de **361k rec/sec**.

### 3. Concorr√™ncia e Threading: O Paradoxo do Paralelismo

Testamos o impacto de `parallelStream()` e Virtual Threads (Java 21).

* **An√°lise:** Para um parser leve, o modo **sequencial** superou o paralelo. Isso ocorreu devido √† redu√ß√£o de conten√ß√£o de locks no `RecordAccumulator` do Kafka Producer e √† elimina√ß√£o do overhead de troca de contexto, permitindo que um √∫nico n√∫cleo de CPU processasse o arquivo de forma linear e ininterrupta.

---

## ‚öôÔ∏è Kafka Producer Tuning ‚Äî Final Configuration & Rationale

All configurations below were defined after iterative benchmarking against a single-node Apache Kafka broker running locally.

| Property | Final Value | Why This Won |
|-----------|-------------|--------------|
| `batch.size` | `524288` (512KB) | Large batches significantly reduced request frequency and improved per-request efficiency. |
| `linger.ms` | `100` | Allowed dense batches to form, shifting the trigger from time-based to size-based dispatch under sustained load. |
| `compression.type` | `lz4` | Fast compression with low CPU overhead, improving network efficiency without becoming CPU-bound. |
| `buffer.memory` | `134217728` (128MB) | Prevented producer-side backpressure during peak accumulation phases. |

---

### üß† Why This Configuration Was Superior

During experimentation, three important dynamics emerged:

#### 1Ô∏è‚É£ Sender Thread Efficiency Ceiling

The Kafka Producer uses a **single internal Sender I/O thread** responsible for:

* Building produce requests  
* Managing batching  
* Sending data to the broker  

Increasing caller concurrency (`parallelStream`, virtual threads, custom executors) did not significantly increase throughput.

This confirmed that the bottleneck was not application-level parallelism but:

> The efficiency of the Producer's internal dispatch path and the broker append pipeline.

Once saturated, adding more threads only introduced contention.

---

#### 2Ô∏è‚É£ Batch Density > Dispatch Frequency

Earlier tests with smaller batch sizes showed that `linger.ms=0` performed better.

However, after increasing:

* `batch.size` to 512KB  
* `buffer.memory` to 128MB  

Setting `linger.ms=100` allowed the producer to:

* Build denser batches  
* Reduce total request count  
* Reduce syscall overhead  
* Reduce broker-side request handling cost  

Result:

* Similar or slightly better throughput  
* Lower CPU usage  
* Higher throughput per CPU cycle  

This configuration was not just faster ‚Äî it was more **efficient**.

---

#### 3Ô∏è‚É£ Compression Trade-Off

* `zstd` provided strong compression but higher CPU cost.
* `lz4` provided the best throughput/CPU balance for localhost.

Under local conditions (low network latency), CPU becomes the limiting factor faster than network bandwidth.

---

# üìà Performance Evolution Timeline

This section summarizes how each experiment shaped the final configuration.

---

## üîπ Phase 1 ‚Äî Baseline Sequential Producer

**Configuration:**

* Default batch  
* Default buffer  
* `linger.ms=0`  
* `zstd` compression  
* 6 partitions  

**Result:** ~340k records/sec  

**Observation:**  
CPU usage peaked but machine cores were not fully saturated.  
Throughput appeared bounded by broker append path and producer dispatch efficiency.

---

## üîπ Phase 2 ‚Äî Partition Scaling (6 ‚Üí 12)

**Result:** ~342k ‚Üí ~361k records/sec (~5.5% gain)

**Conclusion:**  
Increasing partitions improved parallelism at the broker log level, but gains were modest due to:

* Single broker  
* Shared network threads  
* Shared disk subsystem  

Partitioning helps ‚Äî but does not scale linearly on a single node.

---

## üîπ Phase 3 ‚Äî Concurrency Experiments

Tested:

* `parallelStream()`  
* Explicit ForkJoin parallelism tuning  
* Virtual threads with backpressure semaphore  

**Findings:**

* Parallel stream yielded negligible improvement.  
* Virtual threads reduced throughput drastically (~74k records/sec).  
* Sequential processing outperformed parallel under low parsing complexity.

**Conclusion:**

The bottleneck was not parsing or caller concurrency.

It was:

> Producer Sender thread efficiency + broker ingestion path.

Wrapping a non-blocking `send()` call in additional scheduling layers introduced overhead and reduced throughput.

---

## üîπ Phase 4 ‚Äî Serialization Strategy

Compared:

* Raw `byte[]`  
* Binary encoding (Pseudo-Avro simulation)  

**Result:**  
Binary conversion reduced throughput to ~266k records/sec.

**Conclusion:**  

On localhost, CPU cost of serialization outweighed network savings.

The system was CPU-bound, not network-bound.

---

## üîπ Phase 5 ‚Äî Batch & Buffer Scaling (Final Optimization)

Changes:

* `batch.size` ‚Üí 512KB  
* `buffer.memory` ‚Üí 128MB  
* `compression` ‚Üí `lz4`  
* `linger.ms` ‚Üí 100  

**Final Result:**

* ~361k records/sec  
* Lower CPU usage than previous high-throughput configuration  
* Improved efficiency per request  

This configuration shifted the system from:

Request-frequency optimized  
‚Üí Batch-density optimized  

Which proved superior under sustained ingestion load.

---

# üèÅ Final Architectural Insight

This benchmark revealed a key principle:

> In high-throughput ingestion systems, efficiency per request matters more than raw parallelism.

On a single-node broker:

* Throughput is bounded by sender thread efficiency and broker append path.
* Increasing application-level concurrency does not overcome internal Kafka limits.
* Proper batching and memory sizing provide more gains than thread multiplication.

---

## üí° Production Consideration

In a multi-broker cluster environment:

Throughput would scale horizontally via:

* Increasing broker count  
* Increasing partition count  
* Running multiple producer instances  

This would distribute leadership and log append operations across machines, overcoming the single-node ceiling observed in this benchmark.

---

## ‚öñÔ∏è Vis√£o S√™nior: Localhost vs. Ambiente de Nuvem

Este benchmark revela trade-offs essenciais para decis√µes de arquitetura em grandes corpora√ß√µes (como Accenture ou Magalu):

1. **Gargalo de CPU vs. Rede:** No localhost, o objetivo √© reduzir o processamento do Java (CPU-Bound). Na nuvem, o objetivo seria reduzir o tamanho da mensagem (I/O-Bound) via Avro para economizar custos de transfer√™ncia de dados (*egress*).
2. **Efici√™ncia de Mem√≥ria:** O custo de aloca√ß√£o de Strings (via `substring`) √© aceit√°vel em aplica√ß√µes de baixa carga, mas torna-se proibitivo em pipelines de Big Data, onde o processamento orientado a bytes √© a norma.
3. **Escalabilidade:** Escalabilidade da Ingest√£o: Enquanto o modo sequencial venceu localmente (devido ao baixo overhead), em um cen√°rio de nuvem a ingest√£o escalaria atrav√©s do aumento do n√∫mero de parti√ß√µes do t√≥pico e da execu√ß√£o de m√∫ltiplas inst√¢ncias do Producer em paralelo (ex: em um cluster Kubernetes). Isso permitiria distribuir a carga de escrita entre diferentes brokers, superando o limite de vaz√£o de uma √∫nica CPU ou placa de rede.

---

### Como Rodar

1. Certifique-se de ter um broker Kafka em `localhost:9092`.
2. Configure o t√≥pico com **12 parti√ß√µes** para performance m√°xima.
3. O arquivo `books_large.csv` deve estar na raiz ou selecione outro csv.
4. Execute `IngestionBenchmark.java`.
